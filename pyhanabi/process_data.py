import time
import os

import numpy as np
import torch
from torch import nn

from act_group import ActGroup
# from create import create_envs, create_threads
# from eval import evaluate
# import common_utils
import rela
import hanalearn
import pickle
import numpy as np
from collections import namedtuple
from load_data import PrioritizedReplayBuffer
import argparse


def parse_args():
    parser = argparse.ArgumentParser(description="train dqn on hanabi")
    parser.add_argument("--save_dir", type=str, default="exps/exp1")
    parser.add_argument("--method", type=str, default="vdn")
    parser.add_argument("--shuffle_color", type=int, default=0)
    parser.add_argument("--aux_weight", type=float, default=0)
    parser.add_argument("--boltzmann_act", type=int, default=0)
    parser.add_argument("--min_t", type=float, default=1e-3)
    parser.add_argument("--max_t", type=float, default=1e-1)
    parser.add_argument("--num_t", type=int, default=80)
    parser.add_argument("--hide_action", type=int, default=0)
    parser.add_argument("--off_belief", type=int, default=0)
    parser.add_argument("--belief_model", type=str, default="None")
    parser.add_argument("--num_fict_sample", type=int, default=10)
    parser.add_argument("--belief_device", type=str, default="cuda:1")

    parser.add_argument("--load_model", type=str, default="")
    parser.add_argument("--clone_bot", type=str, default="", help="behavior clone loss")
    parser.add_argument("--clone_weight", type=float, default=0.0)
    parser.add_argument("--clone_t", type=float, default=0.02)

    parser.add_argument("--seed", type=int, default=10001)
    parser.add_argument("--gamma", type=float, default=0.999, help="discount factor")
    parser.add_argument(
        "--eta", type=float, default=0.9, help="eta for aggregate priority"
    )
    parser.add_argument("--train_bomb", type=int, default=0)
    parser.add_argument("--eval_bomb", type=int, default=0)
    parser.add_argument("--sad", type=int, default=0)
    parser.add_argument("--num_player", type=int, default=2)

    # optimization/training settings
    parser.add_argument("--lr", type=float, default=6.25e-5, help="Learning rate")
    parser.add_argument("--eps", type=float, default=1.5e-5, help="Adam epsilon")
    parser.add_argument("--grad_clip", type=float, default=5, help="max grad norm")
    parser.add_argument("--num_lstm_layer", type=int, default=2)
    parser.add_argument("--rnn_hid_dim", type=int, default=512)
    parser.add_argument(
        "--net", type=str, default="publ-lstm", help="publ-lstm/ffwd/lstm"
    )

    parser.add_argument("--train_device", type=str, default="cuda:0")
    parser.add_argument("--batchsize", type=int, default=128)
    parser.add_argument("--num_epoch", type=int, default=5000)
    parser.add_argument("--epoch_len", type=int, default=1000)
    parser.add_argument("--num_update_between_sync", type=int, default=2500)

    # DQN settings
    parser.add_argument("--multi_step", type=int, default=3)

    # replay buffer settings
    parser.add_argument("--burn_in_frames", type=int, default=10000)
    parser.add_argument("--replay_buffer_size", type=int, default=100000)

    parser.add_argument(
        "--priority_exponent", type=float, default=0.9, help="alpha in p-replay"
    )

    parser.add_argument(
        "--priority_weight", type=float, default=0.6, help="beta in p-replay"
    )

    parser.add_argument("--max_len", type=int, default=80, help="max seq len")
    parser.add_argument("--prefetch", type=int, default=3, help="#prefetch batch")

    # thread setting
    parser.add_argument("--num_thread", type=int, default=10, help="#thread_loop")
    parser.add_argument("--num_game_per_thread", type=int, default=40)

    # actor setting
    parser.add_argument("--act_base_eps", type=float, default=0.1)
    parser.add_argument("--act_eps_alpha", type=float, default=7)
    parser.add_argument("--act_device", type=str, default="cuda:1")
    parser.add_argument("--actor_sync_freq", type=int, default=10)

    args = parser.parse_args()
    if args.off_belief == 1:
        args.method = "iql"
        args.multi_step = 1
        assert args.net in ["publ-lstm"], "should only use publ-lstm style network"
        assert not args.shuffle_color
    assert args.method in ["vdn", "iql"]
    return args


def save_replay_buffer(replay_buffer, epoch, score):
    # import pdb; pdb.set_trace()
    size = replay_buffer.size()
    data = {'action' : [], 'bootstrap' : [], 'h0' : [], 'obs' : [], 'reward' : [], 'seq_len' : [], 'terminal' : []}

    for i in range(size):
        sample = replay_buffer.get(i)
        # import pdb; pdb.set_trace()
        data['action'].append(sample.action)
        data['bootstrap'].append(sample.bootstrap)
        data['h0'].append(sample.h0)
        data['reward'].append(sample.reward)
        data['seq_len'].append(sample.seq_len)
        data['obs'].append(sample.obs)
        data['terminal'].append(sample.terminal)

    data['epoch'] = epoch
    data['score'] = score
    # print(data)
    # Save the data to a pickle file
    folder_name = "./offline_data/"
    filename = "data_"+str(epoch)+".pickle"
    if os.path.exists(folder_name) == False:
        os.mkdir(folder_name)
    filename = folder_name + filename
    with open(filename, 'wb') as file:
        pickle.dump(data, file)

if __name__ == "__main__":
    # Create a replay buffer
    args = parse_args()
    replay_buffer = PrioritizedReplayBuffer(args)
    replay_buffer.load('/data/kmirakho/offline_data/dataset_rl_1040640_sml.npz')
    import pdb; pdb.set_trace()
    